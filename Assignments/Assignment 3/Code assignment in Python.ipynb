{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "29347c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Import data from the O*NET database, at ISCO-08 occupation level.\n",
    "# The original data uses a version of SOC classification, but the data we load here\n",
    "# are already cross-walked to ISCO-08 using: https://ibs.org.pl/en/resources/occupation-classifications-crosswalks-from-onet-soc-to-isco/\n",
    "\n",
    "# The O*NET database contains information for occupations in the USA, including\n",
    "# the tasks and activities typically associated with a specific occupation.\n",
    "\n",
    "task_data = pd.read_csv(\"C:\\\\Users\\\\X\\\\Downloads\\\\onet_tasks.csv\")\n",
    "# isco08 variable is for occupation codes\n",
    "# the t_* variables are specific tasks conducted on the job\n",
    "\n",
    "# read employment data from Eurostat\n",
    "# These datasets include quarterly information on the number of workers in specific\n",
    "# 1-digit ISCO occupation categories. (Check here for details: https://www.ilo.org/public/english/bureau/stat/isco/isco08/)\n",
    "\n",
    "path:str = \"C:\\\\Users\\\\X\\\\Downloads\\\\Eurostat_employment_isco.xlsx\"\n",
    "\n",
    "def read_excel_sheet(path:str = None, r:int = None) -> dict | None:\n",
    "    if r == None or path == None:\n",
    "        return None\n",
    "    \n",
    "    # dictionary of names\n",
    "    d = {}\n",
    "    for i in range(1, r + 1):\n",
    "        d[f\"isco{i}\"] = pd.read_excel(io=path, sheet_name=f\"ISCO{i}\")\n",
    "    \n",
    "    return d\n",
    "\n",
    "dic = read_excel_sheet(path, 9)\n",
    "\n",
    "# This will calculate worker totals in each of the chosen countries.\n",
    "total_Belgium = dic['isco1'][\"Belgium\"] + dic['isco2'][\"Belgium\"] + dic['isco3'][\"Belgium\"] + dic['isco4'][\"Belgium\"] + dic['isco5'][\"Belgium\"] + dic['isco6'][\"Belgium\"] + dic['isco7'][\"Belgium\"] + dic['isco8'][\"Belgium\"] + dic['isco9'][\"Belgium\"]\n",
    "total_Spain = dic['isco1'][\"Spain\"] + dic['isco2'][\"Spain\"] + dic['isco3'][\"Spain\"] + dic['isco4'][\"Spain\"] + dic['isco5'][\"Spain\"] + dic['isco6'][\"Spain\"] + dic['isco7'][\"Spain\"] + dic['isco8'][\"Spain\"] + dic['isco9'][\"Spain\"]\n",
    "total_Poland = dic['isco1'][\"Poland\"] + dic['isco2'][\"Poland\"] + dic['isco3'][\"Poland\"] + dic['isco4'][\"Poland\"] + dic['isco5'][\"Poland\"] + dic['isco6'][\"Poland\"] + dic['isco7'][\"Poland\"] + dic['isco8'][\"Poland\"] + dic['isco9'][\"Poland\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7663320d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Let's merge all these datasets. We'll need a column that stores the occupation categories:\n",
    "dic['isco1']['ISCO'] = 1\n",
    "dic['isco2']['ISCO'] = 2\n",
    "dic['isco3']['ISCO'] = 3\n",
    "dic['isco4']['ISCO'] = 4\n",
    "dic['isco5']['ISCO'] = 5\n",
    "dic['isco6']['ISCO'] = 6\n",
    "dic['isco7']['ISCO'] = 7\n",
    "dic['isco8']['ISCO'] = 8\n",
    "dic['isco9']['ISCO'] = 9\n",
    "\n",
    "# and this gives us one large file with employment in all occupations.\n",
    "all_data = pd.concat([dic['isco1'], dic['isco2'], dic['isco3'], dic['isco4'], dic['isco5'], dic['isco6'], dic['isco7'], dic['isco8'], dic['isco9']], ignore_index=True)\n",
    "\n",
    "# We have 9 occupations and the same time range for each, so we can add the totals by\n",
    "# adding a vector that is 9 times the previously calculated totals\n",
    "\n",
    "all_data[\"total_Belgium\"] = pd.concat([total_Belgium]*9, ignore_index=True)\n",
    "all_data[\"total_Spain\"] = pd.concat([total_Spain]*9, ignore_index=True)\n",
    "all_data[\"total_Poland\"] = pd.concat([total_Poland]*9, ignore_index=True)\n",
    "\n",
    "# And this will give us shares of each occupation among all workers in a period-country\n",
    "all_data['share_Belgium'] = all_data['Belgium'] / all_data['total_Belgium']\n",
    "all_data['share_Spain'] = all_data['Spain'] / all_data['total_Spain']\n",
    "all_data['share_Poland'] = all_data['Poland'] / all_data['total_Poland']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2eacc72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's look at the task data. We want the first digit of the ISCO variable only\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "task_data[\"isco08_1dig\"] = task_data[\"isco08\"].astype(str).str[:1].astype(int)\n",
    "\n",
    "# And we'll calculate the mean task values at a 1-digit level \n",
    "# (more on what these tasks are below)\n",
    "aggdata = task_data.groupby([\"isco08_1dig\"]).mean()\n",
    "aggdata = aggdata.drop(columns=[\"isco08\"])\n",
    "\n",
    "# We'll be interested in tracking the intensity of Non-routine cognitive analytical tasks\n",
    "# Using a framework reminiscent of the work by David Autor.\n",
    "\n",
    "#These are the ones we're interested in:\n",
    "# Non-routine cognitive analytical\n",
    "# 4.A.2.a.4 Analyzing Data or Information\n",
    "# 4.A.2.b.2 Thinking Creatively\n",
    "# 4.A.4.a.1 Interpreting the Meaning of Information for Others\n",
    "\n",
    "#Let's combine the data.\n",
    "combined = pd.merge(all_data, aggdata, left_on='ISCO', right_on='isco08_1dig', how='left')\n",
    "# Traditionally, the first step is to standardise the task values using weights \n",
    "# defined by share of occupations in the labour force. This should be done separately\n",
    "# for each country. Standardisation -> getting the mean to 0 and std. dev. to 1.\n",
    "# Let's do this for each of the variables that interests us:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f1e4a76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import gmean\n",
    "\n",
    "shares = ['Belgium', 'Poland', 'Spain']\n",
    "std_shares = [\"std_Belgium_t_4A2a4\", \"std_Poland_t_4A2a4\", \"std_Spain_t_4A2a4\"]\n",
    "combineds = [\"t_4A2a4\", \"t_4A2b2\", \"t_4A4a1\"]\n",
    "\n",
    "def load_temp(share_c:list[str] = None, std_c:list[str] = None, combined_c:list[str] = None) -> None:\n",
    "\n",
    "    for x in range(len(share_c)):\n",
    "        for y in range(len(std_c)):\n",
    "            for z in range(len(combined_c)):\n",
    "                temp_mean=np.average(combined[combined_c[z]],weights=combined[share_c[x]])\n",
    "                temp_sd=np.sqrt(np.average((combined[combined_c[z]]-temp_mean)**2,weights=combined[share_c[x]]))\n",
    "                combined[std_c[y]]=(combined[combined_c[z]]-temp_mean)/temp_sd\n",
    "\n",
    "load_temp(shares, std_shares, combineds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b853c49",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'std_Belgium_t_4A2b2'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3804\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3805\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3806\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mindex.pyx:167\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mindex.pyx:196\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: 'std_Belgium_t_4A2b2'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[44]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# The next step is to calculate the `classic` task content intensity, i.e.\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# how important is a particular general task content category in the workforce\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Here, we're looking at non-routine cognitive analytical tasks, as defined\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# by David Autor and Darron Acemoglu:\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m stats\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m combined[\u001b[33m\"\u001b[39m\u001b[33mBelgium_NRCA\u001b[39m\u001b[33m\"\u001b[39m] = combined[\u001b[33m\"\u001b[39m\u001b[33mstd_Belgium_t_4A2a4\u001b[39m\u001b[33m\"\u001b[39m] + \u001b[43mcombined\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstd_Belgium_t_4A2b2\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m + combined[\u001b[33m\"\u001b[39m\u001b[33mstd_Belgium_t_4A4a1\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m      9\u001b[39m combined[\u001b[33m\"\u001b[39m\u001b[33mPoland_NRCA\u001b[39m\u001b[33m\"\u001b[39m] = combined[\u001b[33m\"\u001b[39m\u001b[33mstd_Poland_t_4A2a4\u001b[39m\u001b[33m\"\u001b[39m] + combined[\u001b[33m\"\u001b[39m\u001b[33mstd_Poland_t_4A2b2\u001b[39m\u001b[33m\"\u001b[39m] + combined[\u001b[33m\"\u001b[39m\u001b[33mstd_Poland_t_4A4a1\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     10\u001b[39m combined[\u001b[33m\"\u001b[39m\u001b[33mSpain_NRCA\u001b[39m\u001b[33m\"\u001b[39m] = combined[\u001b[33m\"\u001b[39m\u001b[33mstd_Spain_t_4A2a4\u001b[39m\u001b[33m\"\u001b[39m] + combined[\u001b[33m\"\u001b[39m\u001b[33mstd_Spain_t_4A2b2\u001b[39m\u001b[33m\"\u001b[39m] + combined[\u001b[33m\"\u001b[39m\u001b[33mstd_Spain_t_4A4a1\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\pandas\\core\\frame.py:4102\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4100\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.columns.nlevels > \u001b[32m1\u001b[39m:\n\u001b[32m   4101\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getitem_multilevel(key)\n\u001b[32m-> \u001b[39m\u001b[32m4102\u001b[39m indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4103\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[32m   4104\u001b[39m     indexer = [indexer]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3807\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   3808\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc.Iterable)\n\u001b[32m   3809\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[32m   3810\u001b[39m     ):\n\u001b[32m   3811\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[32m-> \u001b[39m\u001b[32m3812\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   3813\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   3814\u001b[39m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[32m   3815\u001b[39m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[32m   3816\u001b[39m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[32m   3817\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n",
      "\u001b[31mKeyError\u001b[39m: 'std_Belgium_t_4A2b2'"
     ]
    }
   ],
   "source": [
    "# The next step is to calculate the `classic` task content intensity, i.e.\n",
    "# how important is a particular general task content category in the workforce\n",
    "# Here, we're looking at non-routine cognitive analytical tasks, as defined\n",
    "# by David Autor and Darron Acemoglu:\n",
    "\n",
    "from scipy import stats\n",
    "combined[\"Belgium_NRCA\"] = combined[\"std_Belgium_t_4A2a4\"] + combined[\"std_Belgium_t_4A2b2\"] + combined[\"std_Belgium_t_4A4a1\"]\n",
    "combined[\"Poland_NRCA\"] = combined[\"std_Poland_t_4A2a4\"] + combined[\"std_Poland_t_4A2b2\"] + combined[\"std_Poland_t_4A4a1\"]\n",
    "combined[\"Spain_NRCA\"] = combined[\"std_Spain_t_4A2a4\"] + combined[\"std_Spain_t_4A2b2\"] + combined[\"std_Spain_t_4A4a1\"]\n",
    "\n",
    "# And we standardise NRCA in a similar way.\n",
    "\n",
    "def combine(combs, shares, ):\n",
    "\n",
    "    temp_mean = np.average(combined[\"Belgium_NRCA\"], weights=combined[\"share_Belgium\"])\n",
    "    temp_sd = np.sqrt(np.average((combined[\"Belgium_NRCA\"] - temp_mean)**2, weights=combined[\"share_Belgium\"]))\n",
    "    combined[\"std_Belgium_NRCA\"] = (combined[\"Belgium_NRCA\"] - temp_mean) / temp_sd\n",
    "\n",
    "temp_mean = np.average(combined[\"Poland_NRCA\"], weights=combined[\"share_Poland\"])\n",
    "temp_sd = np.sqrt(np.average((combined[\"Poland_NRCA\"] - temp_mean)**2, weights=combined[\"share_Poland\"]))\n",
    "combined[\"std_Poland_NRCA\"] = (combined[\"Poland_NRCA\"] - temp_mean) / temp_sd\n",
    "\n",
    "temp_mean = np.average(combined[\"Spain_NRCA\"], weights=combined[\"share_Spain\"])\n",
    "temp_sd = np.sqrt(np.average((combined[\"Spain_NRCA\"] - temp_mean)**2, weights=combined[\"share_Spain\"]))\n",
    "combined[\"std_Spain_NRCA\"] = (combined[\"Spain_NRCA\"] - temp_mean) / temp_sd\n",
    "\n",
    "\n",
    "# Finally, to track the changes over time, we have to calculate a country-level mean\n",
    "# Step 1: multiply the value by the share of such workers.\n",
    "combined[\"multip_Spain_NRCA\"] = combined[\"std_Spain_NRCA\"] * combined[\"share_Spain\"]\n",
    "combined[\"multip_Belgium_NRCA\"] = combined[\"std_Belgium_NRCA\"] * combined[\"share_Belgium\"]\n",
    "combined[\"multip_Poland_NRCA\"] = combined[\"std_Poland_NRCA\"] * combined[\"share_Poland\"]\n",
    "\n",
    "# Step 2: sum it up (it basically becomes another weighted mean)\n",
    "\n",
    "agg_Spain = combined.groupby([\"TIME\"])[\"multip_Spain_NRCA\"].sum().reset_index()\n",
    "agg_Belgium = combined.groupby([\"TIME\"])[\"multip_Belgium_NRCA\"].sum().reset_index()\n",
    "agg_Poland = combined.groupby([\"TIME\"])[\"multip_Poland_NRCA\"].sum().reset_index()\n",
    "\n",
    "# We can plot it now!\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(agg_Poland[\"TIME\"], agg_Poland[\"multip_Poland_NRCA\"])\n",
    "plt.xticks(range(0, len(agg_Poland), 3), agg_Poland[\"TIME\"][::3])\n",
    "plt.show()\n",
    "\n",
    "plt.plot(agg_Spain[\"TIME\"], agg_Spain[\"multip_Spain_NRCA\"])\n",
    "plt.xticks(range(0, len(agg_Spain), 3), agg_Spain[\"TIME\"][::3])\n",
    "plt.show()\n",
    "\n",
    "plt.plot(agg_Belgium[\"TIME\"], agg_Belgium[\"multip_Belgium_NRCA\"])\n",
    "plt.xticks(range(0, len(agg_Belgium), 3), agg_Belgium[\"TIME\"][::3])\n",
    "plt.show()\n",
    "\n",
    "# If this code gets automated and cleaned properly,\n",
    "#  you should be able to easily add other countries as well as other tasks.\n",
    "# E.g.:\n",
    "\n",
    "# Routine manual\n",
    "# 4.A.3.a.3\tControlling Machines and Processes\n",
    "# 4.C.2.d.1.i\tSpend Time Making Repetitive Motions\n",
    "# 4.C.3.d.3\tPace Determined by Speed of Equipment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9e70f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
